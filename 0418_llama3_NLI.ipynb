{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PWEI07/context_distiliation/blob/main/0418_llama3_NLI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puic9MMOdefj",
        "outputId": "baefa44b-0de4-4021-975e-384c1dc8c170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!chmod 755 -R /content/drive/MyDrive/colab_env/bin\n",
        "# !mkdir Mixtral-8x7B-Instruct-v0.1-GPTQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RfcKYmAETEnu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AUiR0AP5jVV",
        "outputId": "0b7791bd-045a-4103-a964-832769345aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "# todo modify the two paths below to where you store the virtual environment I shared with you.\n",
        "%cd '/content/drive/MyDrive'\n",
        "# !mkdir Mistral-7B-Instruct-v0.2\n",
        "!source /content/drive/MyDrive/colab_env/bin/activate\n",
        "import sys\n",
        "packate_path = r'/content/drive/MyDrive/colab_env/lib/python3.10/site-packages'\n",
        "sys.path.append(packate_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8Ztz2X3Fp-Cw"
      },
      "outputs": [],
      "source": [
        "DATA_SET_NAME = \"pwei07/cqa\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHkMylfBZxLP"
      },
      "source": [
        "## Install package and LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "421annm_74I5",
        "outputId": "7a9d2671-e724-449b-a4ea-6b01d2cea03d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import huggingface_hub\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "huggingface_hub.login(os.environ[\"HF_TOKEN\"], add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sz5qBOa2Pb4X"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !source /content/drive/MyDrive/colab_env/bin/activate; pip install accelerate==0.28.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DOzNAVZ26_1E"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --local-dir Meta-Llama-3-8B-Instruct --local-dir-use-symlinks False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UWS8evS3AGht"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model_id = r'/content/drive/MyDrive/gemma-7b-it'\n",
        "model_id = r'/content/drive/MyDrive/Meta-Llama-3-8B-Instruct'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNuMOajf4qN3"
      },
      "source": [
        "# Load LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r3fCEhQ49w8P"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "import gc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nFJKG6WhGs9H"
      },
      "outputs": [],
      "source": [
        "def clean_gpu_cache():\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UtfWbqLdvqMb"
      },
      "outputs": [],
      "source": [
        "clean_gpu_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c93_ym7BMUuI",
        "outputId": "cf05196d-2594-4433-dfcc-c2be9fa36e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 21 14:57:16 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0              26W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9V-5RbpP2ZQ2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "# import os, torch, wandb, platform, warnings\n",
        "import os, torch, platform, warnings\n",
        "\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant= False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "324be0d68baa4784a4d28ec00a3707fa",
            "dfb8ff664e724187a0d21c6c819a10bf",
            "a3f7138cab8449cea028cbb626c9db36",
            "44f65edeb8f34cf8bd28c3ebac674714",
            "bec1e235c8a94fc8b6ef5884244a3172",
            "609edd1858b84104ab8c311c2f71bc15",
            "26403a33e75645c88808175dcc877d6e",
            "95c4f7e25ac545e585bdc252f161133a",
            "9c44ba4ce93d4d17a912a4aaeda05b8c",
            "0c4c49cf4b7549578bdb08a11538032a",
            "e7e70f049fc04b94ab79b70f1e86f0fd"
          ]
        },
        "id": "DhJQBbBwiuW6",
        "outputId": "d1ad3b52-4842-41b5-81b9-c7689823b455"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "324be0d68baa4784a4d28ec00a3707fa"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    # device_map={\"\": 0},\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        "\n",
        ")\n",
        "model.config.use_cache = True # silence the warnings. Please re-enable for inference!\n",
        "# model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()\n",
        "# Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia2MDPO5iv38",
        "outputId": "961cca2c-97ab-4de1-9a78-cd9f932f0fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.add_eos_token = True\n",
        "# tokenizer.add_bos_token, tokenizer.add_eos_token\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6JMfgyJSproy"
      },
      "outputs": [],
      "source": [
        "# for llama 3\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cG_6pBRtV4x"
      },
      "source": [
        "# Get Data and Process it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "34eGvIxMgqsE"
      },
      "outputs": [],
      "source": [
        "def format_label_question(example):\n",
        "  input = example['input']\n",
        "  chat = [{\"role\": \"system\", \"content\": \"Use common sense reasoning to answer question by choosing 1 from 5 choices (delimited with XML tags) and stating your reason\\n\"},\n",
        "        {\"role\": \"user\", \"content\": f\"<question>{input.split('Answer Choices:')[0]}<\\question>\\n<choices>{input.split('Answer Choices:')[-1]}<\\choices>\"}\n",
        "    ]\n",
        "  # f\"Pick 1 from the 5 answers, then state the reason : {example['input'][0]}\\nAnswer:\"\n",
        "  chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "  example[\"label_question\"] = chat_text\n",
        "\n",
        "  return example\n",
        "\n",
        "\n",
        "# def format_label_question(example):\n",
        "#     example[\"label_question\"] = f\"Question: {example['input']}\\nAnswer:\"\n",
        "#     return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "37SsMnyV34BR"
      },
      "outputs": [],
      "source": [
        "def format_label_pair(example):\n",
        "  input = example['input']\n",
        "  # chat = {\"prompt\": f\"Use common sense reasoning to answer question by choosing 1 from 5 choices\\n<question>{input.split('Answer Choices:')[0]}<\\question>\\n<choices>{input.split('Answer Choices:')[-1]}<\\choices>\",\n",
        "  #        \"completion\": example['label']}\n",
        "\n",
        "  # chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
        "  # chat = f\"\"\"prompt: Use common sense reasoning to answer question by choosing 1 from 5 choices\\n<question>{input.split('Answer Choices:')[0]}<\\question>\\n<choices>{input.split('Answer Choices:')[-1]}<\\choices>\\ncompletion: {example['label']}\"\"\"\n",
        "  chat = f'''Below is an instruction that describes a task, paired with an input. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Use common sense reasoning to answer question by choosing 1 from 5 choices\n",
        "### Input:\n",
        "<question>{input.split('Answer Choices:')[0]}<\\question>\\n<choices>{input.split('Answer Choices:')[-1]}<\\choices>\n",
        "### Response:\n",
        "{example['label']}\n",
        "'''\n",
        "  example[\"label_pair\"] = chat\n",
        "\n",
        "  return example\n",
        "\n",
        "\n",
        "# def format_label_pair(example):\n",
        "#     example['label_pair'] = f\"Question: {example['input']}\\nAnswer: {example['label']}\"\n",
        "#     return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NUbNb9nigxpM"
      },
      "outputs": [],
      "source": [
        "def format_rationale_question(example):\n",
        "  input = example['input']\n",
        "  chat = [\n",
        "        {\"role\": \"user\", \"content\": f\"Given a question, choices, and answer, state the reason for the answer\\n<question>{input.split('Answer Choices:')[0]}<\\question>\\n<choices>{input.split('Answer Choices:')[-1]}<\\choices>\\n<\\question>\\n<answer>{example['llm_label']}<\\answer>\"}\n",
        "    ]\n",
        "  chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "  example[\"rationale_question\"] = chat_text\n",
        "\n",
        "  return example\n",
        "\n",
        "# def format_rationale_question(example):\n",
        "#     example[\"rationale_question\"] = f\"Question: {example['input']}\\nAnswer: {example['llm_label']}\\nAnswer's reasoning:\"\n",
        "#     return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "V75Xt4rStM0U"
      },
      "outputs": [],
      "source": [
        "def format_rationale_pair(example):\n",
        "  input = example['input']\n",
        "  # chat = [\n",
        "  #       {\"role\": \"user\", \"content\": f\"Given a question, choices, and answer, state the reason for the answer\\n<question>{input.split('Answer Choices:')[0]}<\\question>\\n<choices>{input.split('Answer Choices:')[-1]}<\\choices>\\n<\\question>\\n<answer>{example['llm_label']}<\\answer>\"},\n",
        "  #       {\"role\": \"assistant\", \"content\": example['rationale']}\n",
        "  #   ]\n",
        "  chat = f'''Below is an instruction that describes a task, paired with an input. Write a response that appropriately completes the task.\n",
        "### Instruction:\n",
        "Given question, choices, and answer (all delimited with XML tags), state the reason for the answer\n",
        "### Input:\n",
        "<question>{input.split('Answer Choices:')[0]}<\\question>\\n<choices>{input.split('Answer Choices:')[-1]}<\\choices>\\n<answer>{example['llm_label']}<\\answer>\n",
        "### Response:\n",
        "{example['rationale']}\n",
        "'''\n",
        "  example[\"rationale_pair\"] = chat\n",
        "  # chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
        "  # example[\"rationale_pair\"] = chat_text\n",
        "\n",
        "  return example\n",
        "\n",
        "# def format_rationale_pair(example):\n",
        "#     example[\"rationale_pair\"] = f\"Question: {example['input']}\\nAnswer: {example['llm_label']}\\nAnswer's reasoning: {example['rationale']}\"\n",
        "#     return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "g6QaoO_A5U5R"
      },
      "outputs": [],
      "source": [
        "def get_dataset(dataset_name=DATA_SET_NAME):\n",
        "  data = load_dataset(dataset_name, use_auth_token=True)\n",
        "  data = data.map(format_label_question)\n",
        "  data = data.map(format_label_pair)\n",
        "  data = data.map(format_rationale_question)\n",
        "  data = data.map(format_rationale_pair)\n",
        "  return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Pn2QbNqg276P"
      },
      "outputs": [],
      "source": [
        "def format_label_question_anli1(example):\n",
        "  input = example['input']\n",
        "  chat = [{\"role\": \"system\", \"content\": \"As a natural language inference expert, choose 1 from these 3: 'contradiction', 'entailment', 'neutral' that best describes the relationship between premise and hypothesis, then state your reason\"},\n",
        "          {\"role\": \"user\", \"content\": f\"<premise>{input.split('</s>')[0]}</premise>\\n<hypothesis>{input.split('</s>')[-1]}</hypothesis>\"}]\n",
        "  # chat = [\n",
        "  #       {\"role\": \"user\", \"content\": f\"Given a premise and a hypothesis (delimited with XML \"\n",
        "  #                                     f\"tags). Choose 1 from these 3: 'contradiction', 'entailment', 'neutral' based \"\n",
        "  #                                     f\"on inference relationship between premise and hypothesis and state your reason\\n<premise>{input.split('</s>')[0]}</premise>\\n<hypothesis>{input.split('</s>')[-1]}</hypothesis>\"},\n",
        "  #   ]\n",
        "  chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "  example[\"label_question\"] = chat_text\n",
        "  return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OH6TQYrK8xwa"
      },
      "outputs": [],
      "source": [
        "def format_label_pair_anli1(example):\n",
        "  input = example['input']\n",
        "  chat = {\"prompt\": f\"As a natural language inference expert, choose 1 from these 3: 'contradiction', 'entailment', 'neutral' that best describes the relationship between premise and hypothesis\",\n",
        "         \"completion\": example['label']}\n",
        "  # chat = [\n",
        "  #       {\"role\": \"user\", \"content\": \"You will be provided with a premise and then a hypothesis (delimited with XML \"\n",
        "  #                                     \"tags). Choose 1 from these 3: 'contradiction', 'entailment', 'neutral' based \"\n",
        "  #                                     f\"on inference relationship between premise and hypothesis and state your reason<premise>{input.split('</s>')[0]}</premise>\\n<hypothesis>{input.split('</s>')[-1]}</hypothesis>\"},\n",
        "  #       {\"role\": \"assistant\", \"content\": example['label']}\n",
        "  #   ]\n",
        "  # chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
        "  example[\"label_pair\"] = chat\n",
        "\n",
        "  return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-IcUH8t4FbI4"
      },
      "outputs": [],
      "source": [
        "def format_rationale_question_anli1(example):\n",
        "  input = example['input']\n",
        "  chat = [\n",
        "        {\"role\": \"user\", \"content\": f\"Given a premise, a hypothesis, and relationship between them (delimited with XML tags). State the reason for this relationship\\n<premise>{input.split('</s>')[0]}</premise>\\n<hypothesis>{input.split('</s>')[-1]}</hypothesis>\\n<relationship>{example['label']}</relationship>\"},\n",
        "    ]\n",
        "  chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "  example[\"rationale_question\"] = chat_text\n",
        "\n",
        "  return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lDrT-g8bFbI5"
      },
      "outputs": [],
      "source": [
        "def format_rationale_pair_anli1(example):\n",
        "  input = example['input']\n",
        "  chat = [\n",
        "        {\"role\": \"user\", \"content\": f\"Given a premise, a hypothesis, and relationship between them (delimited with XML tags). State the reason for this relationship\\n<premise>{input.split('</s>')[0]}</premise>\\n<hypothesis>{input.split('</s>')[-1]}</hypothesis>\\n<relationship>{example['llm_label']}</relationship>\"},\n",
        "        {\"role\": \"assistant\", \"content\": example['rationale']}\n",
        "    ]\n",
        "  chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
        "\n",
        "  example[\"rationale_pair\"] = chat_text\n",
        "  return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "M0SvF-BwNUy7"
      },
      "outputs": [],
      "source": [
        "def get_dataset_anli1(dataset_name=DATA_SET_NAME):\n",
        "  data = load_dataset(dataset_name, use_auth_token=True)\n",
        "  data = data.map(format_label_question_anli1)\n",
        "  data = data.map(format_label_pair_anli1)\n",
        "  data = data.map(format_rationale_question_anli1)\n",
        "  data = data.map(format_rationale_pair_anli1)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AphzRdMOzx4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b62f98-5369-4a34-9202-56014158d6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/colab_env/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "data = get_dataset()\n",
        "# print(data['train'][:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-1ljo4kAqMRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d906c84f-646d-488a-93ac-6865349a7938"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'A kid is studying new words at home in the kitchen, where could a dictionary be?\\nAnswer Choices:\\n(a) library\\n(b) table\\n(c) bookstore\\n(d) shelf\\n(e) classroom',\n",
              " 'label': 'table',\n",
              " 'llm_label': 'library',\n",
              " 'rationale': 'The answer must be a place where a dictionary can be found. Of the above choices, only a library and a classroom have dictionaries.',\n",
              " 'label_question': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nUse common sense reasoning to answer question by choosing 1 from 5 choices (delimited with XML tags) and stating your reason<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n<question>A kid is studying new words at home in the kitchen, where could a dictionary be?\\n<\\\\question>\\n<choices>\\n(a) library\\n(b) table\\n(c) bookstore\\n(d) shelf\\n(e) classroom<\\\\choices><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
              " 'label_pair': 'Below is an instruction that describes a task, paired with an input. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nUse common sense reasoning to answer question by choosing 1 from 5 choices\\n### Input:\\n<question>A kid is studying new words at home in the kitchen, where could a dictionary be?\\n<\\\\question>\\n<choices>\\n(a) library\\n(b) table\\n(c) bookstore\\n(d) shelf\\n(e) classroom<\\\\choices>\\n### Response:\\ntable\\n',\n",
              " 'rationale_question': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGiven a question, choices, and answer, state the reason for the answer\\n<question>A kid is studying new words at home in the kitchen, where could a dictionary be?\\n<\\\\question>\\n<choices>\\n(a) library\\n(b) table\\n(c) bookstore\\n(d) shelf\\n(e) classroom<\\\\choices>\\n<\\\\question>\\n<answer>library<\\x07nswer><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
              " 'rationale_pair': 'Below is an instruction that describes a task, paired with an input. Write a response that appropriately completes the task.\\n### Instruction:\\nGiven question, choices, and answer (all delimited with XML tags), state the reason for the answer\\n### Input:\\n<question>A kid is studying new words at home in the kitchen, where could a dictionary be?\\n<\\\\question>\\n<choices>\\n(a) library\\n(b) table\\n(c) bookstore\\n(d) shelf\\n(e) classroom<\\\\choices>\\n<answer>library<\\x07nswer>\\n### Response:\\nThe answer must be a place where a dictionary can be found. Of the above choices, only a library and a classroom have dictionaries.\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "data['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cxWK4BdREKmX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "data_anli1 = get_dataset_anli1(\"pwei07/anli1\")\n",
        "# data_esnli = get_dataset_anli1(\"pwei07/esnli\")\n",
        "\n",
        "# data_svamp = get_dataset(\"pwei07/svamp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3iM6nSlEhHc"
      },
      "source": [
        "# Evaluate Palm Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_Aazm4FsFvJP"
      },
      "outputs": [],
      "source": [
        "def evaluate_prediction(dataset, evaluation=False, pred_col='llm_label'):\n",
        "\n",
        "    # Convert the dataset to a pandas DataFrame\n",
        "    df = dataset.to_pandas()\n",
        "    def safe_evaluation(row):\n",
        "      try:\n",
        "          return eval(str(row['label'])) == eval(str(row[pred_col]))\n",
        "      except:\n",
        "          return False\n",
        "    # Check if 'label' matches 'llm_label'\n",
        "    if not evaluation:\n",
        "      df['is_correct'] = df.apply(lambda x: x['label'].lower() in x[pred_col].lower(), axis=1)\n",
        "    else:\n",
        "    # Apply the function across the DataFrame rows\n",
        "      df['is_correct'] = df.apply(safe_evaluation, axis=1)\n",
        "\n",
        "    # Calculate the accuracy as the percentage of correct predictions\n",
        "    accuracy = df['is_correct'].mean() * 100\n",
        "\n",
        "    # Extract rows where the condition does not hold\n",
        "    incorrect_predictions = df[~df['is_correct']]\n",
        "\n",
        "    # Return accuracy and the incorrect predictions DataFrame\n",
        "    return accuracy, incorrect_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ooDol7pIh-d9"
      },
      "outputs": [],
      "source": [
        "palm_accuracy_cqa = evaluate_prediction(data['valid'], evaluation=False)\n",
        "palm_accuracy_anli1 = evaluate_prediction(data_anli1['valid'], evaluation=False)\n",
        "# palm_accuracy_esnli = evaluate_prediction(data_esnli['valid'], evaluation=False)\n",
        "# palm_accuracy_svamp = evaluate_prediction(data_svamp['valid'], evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "yuzIpCcHiAR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1354502e-0449-4260-9125-d55d466b7bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "palm_accuracy_cqa 76.0\n",
            "palm_accuracy_anli1 66.8\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"palm_accuracy_cqa\", palm_accuracy_cqa[0])\n",
        "print(\"palm_accuracy_anli1\", palm_accuracy_anli1[0])\n",
        "# print(\"palm_accuracy_esnli\", palm_accuracy_esnli[0])\n",
        "# print(\"palm_accuracy_svamp\", palm_accuracy_svamp[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akfUrJxwHc5E"
      },
      "source": [
        "# test LLM without chat template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DuNUPuXZ2lS4"
      },
      "outputs": [],
      "source": [
        "def get_llm_answer(text, max_new_tokens=None):\n",
        "  temp_encoding = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True).to('cuda')\n",
        "  temp_token_id = temp_encoding['input_ids']\n",
        "  # temp_attention_mask = temp_encoding['attention_mask'].to(device)\n",
        "  # print(temp_attention_mask)\n",
        "  # outputs = model.generate(input_ids=temp_token_id.unsqueeze(0), attention_mask=temp_attention_mask)\n",
        "  outputs = model.generate(**temp_encoding, eos_token_id=terminators, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
        "  # print(temp_token_id.shape)\n",
        "  # print(outputs)\n",
        "  return tokenizer.batch_decode(outputs[:,temp_token_id.shape[1]:], skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "aNFiHYcyzmGi"
      },
      "outputs": [],
      "source": [
        "# example = data['train'][:1]\n",
        "# text_temp = f\"Pick 1 from the 5 answers, then state the reason : {example['input'][0]}\\nAnswer:\"\n",
        "# print(text_temp)\n",
        "# print(get_llm_answer(text_temp,max_new_tokens=30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Bm3V1xfm42bC"
      },
      "outputs": [],
      "source": [
        "# text_temp = f\"Question: {example['input'][0]}\\nAnswer: {example['label'][0]}\\nAnswer's reasoning:\"\n",
        "# get_llm_answer(text_temp, max_new_tokens=64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOo2V8KzKPJQ"
      },
      "source": [
        "# test LLM with chat template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "MWYkHb-e9x2N"
      },
      "outputs": [],
      "source": [
        "def apply_chat_template(text):\n",
        "  chat = [\n",
        "    {\"role\": \"user\", \"content\": text},\n",
        "    # {\"role\": \"assistant\", \"content\": \"The sun.\"}\n",
        "  ]\n",
        "  chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "  # print(chat_text)\n",
        "  chat_text = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
        "  # print(chat_text)\n",
        "  return chat_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PyXxZD3J5k2y"
      },
      "outputs": [],
      "source": [
        "def get_llm_answer_w_chat_template(text,  max_new_tokens=8):\n",
        "  inputs = apply_chat_template(text)\n",
        "  outputs = model.generate(inputs, max_new_tokens=max_new_tokens)\n",
        "  # print(outputs.logits.shape)\n",
        "  return tokenizer.batch_decode(outputs[:,inputs.shape[1]:], skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "82pUn-nF9JYh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1bf2e1a4-2fd2-4d72-8b5a-766330c618d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I would choose \"contradiction\" to describe the relationship between the premise and'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# text_temp = f\"Question: {example['input'][0]}\\nAnswer:\"\n",
        "get_llm_answer(data_anli1['train'][0]['label_question'], max_new_tokens=16)\n",
        "# get_llm_answer_w_chat_template(text_temp,  max_new_tokens=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rve8qALk4fX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05747ea6-e9a7-4aef-a87f-5a0aef516786"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'entailment',\n",
              " 'llm_label': 'entailment',\n",
              " 'rationale': 'The premise mentions that the system has 4 urban routes.',\n",
              " 'input': 'The Parma trolleybus system (Italian: \"Rete filoviaria di Parma\" ) forms part of the public transport network of the city and \"comune\" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.</s>The trolleybus system has over 2 urban routes',\n",
              " 'label_question': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nAs a natural language inference expert, choose 1 from these 3: \\'contradiction\\', \\'entailment\\', \\'neutral\\' that best describes the relationship between premise and hypothesis, then state your reason<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n<premise>The Parma trolleybus system (Italian: \"Rete filoviaria di Parma\" ) forms part of the public transport network of the city and \"comune\" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.</premise>\\n<hypothesis>The trolleybus system has over 2 urban routes</hypothesis><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
              " 'label_pair': {'completion': 'entailment',\n",
              "  'prompt': \"As a natural language inference expert, choose 1 from these 3: 'contradiction', 'entailment', 'neutral' that best describes the relationship between premise and hypothesis\"},\n",
              " 'rationale_question': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGiven a premise, a hypothesis, and relationship between them (delimited with XML tags). State the reason for this relationship\\n<premise>The Parma trolleybus system (Italian: \"Rete filoviaria di Parma\" ) forms part of the public transport network of the city and \"comune\" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.</premise>\\n<hypothesis>The trolleybus system has over 2 urban routes</hypothesis>\\n<relationship>entailment</relationship><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
              " 'rationale_pair': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGiven a premise, a hypothesis, and relationship between them (delimited with XML tags). State the reason for this relationship\\n<premise>The Parma trolleybus system (Italian: \"Rete filoviaria di Parma\" ) forms part of the public transport network of the city and \"comune\" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.</premise>\\n<hypothesis>The trolleybus system has over 2 urban routes</hypothesis>\\n<relationship>entailment</relationship><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe premise mentions that the system has 4 urban routes.<|eot_id|>'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "data_anli1['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KboszYdAj5Ir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0cd8927f-efec-4cec-9866-dad0afb1f06e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<answer>(d) shelf</answer>\\n\\nReason: Since the kid is studying new words at home in the kitchen, it's likely that the dictionary is placed\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "get_llm_answer(data['train'][0]['label_question'],max_new_tokens=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Obz1p1hN4M-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a416651-e711-49f9-8717-28839cc81828"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'entailment',\n",
              " 'llm_label': 'entailment',\n",
              " 'rationale': 'The premise mentions that the system has 4 urban routes.',\n",
              " 'input': 'The Parma trolleybus system (Italian: \"Rete filoviaria di Parma\" ) forms part of the public transport network of the city and \"comune\" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.</s>The trolleybus system has over 2 urban routes',\n",
              " 'label_question': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nAs a natural language inference expert, choose 1 from these 3: \\'contradiction\\', \\'entailment\\', \\'neutral\\' that best describes the relationship between premise and hypothesis, then state your reason<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n<premise>The Parma trolleybus system (Italian: \"Rete filoviaria di Parma\" ) forms part of the public transport network of the city and \"comune\" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.</premise>\\n<hypothesis>The trolleybus system has over 2 urban routes</hypothesis><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
              " 'label_pair': {'completion': 'entailment',\n",
              "  'prompt': \"As a natural language inference expert, choose 1 from these 3: 'contradiction', 'entailment', 'neutral' that best describes the relationship between premise and hypothesis\"},\n",
              " 'rationale_question': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGiven a premise, a hypothesis, and relationship between them (delimited with XML tags). State the reason for this relationship\\n<premise>The Parma trolleybus system (Italian: \"Rete filoviaria di Parma\" ) forms part of the public transport network of the city and \"comune\" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.</premise>\\n<hypothesis>The trolleybus system has over 2 urban routes</hypothesis>\\n<relationship>entailment</relationship><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
              " 'rationale_pair': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGiven a premise, a hypothesis, and relationship between them (delimited with XML tags). State the reason for this relationship\\n<premise>The Parma trolleybus system (Italian: \"Rete filoviaria di Parma\" ) forms part of the public transport network of the city and \"comune\" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.</premise>\\n<hypothesis>The trolleybus system has over 2 urban routes</hypothesis>\\n<relationship>entailment</relationship><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe premise mentions that the system has 4 urban routes.<|eot_id|>'}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# get_llm_answer(data_esnli['train'][0]['rationale_question'],max_new_tokens=32)\n",
        "data_anli1['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkopUJrm9Vwt"
      },
      "source": [
        "# Test the untuned model's performance on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "qpFA9AKcadrh"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_accuracy(data, n=50, chat=False, max_new_tokens=30, evaluation=False):\n",
        "  pred = []\n",
        "  for i in range(len(data)):\n",
        "    if not chat:\n",
        "      pred.append(get_llm_answer(data[i]['label_question'],  max_new_tokens=max_new_tokens))\n",
        "    else:\n",
        "      pred.append(get_llm_answer_w_chat_template(data[i]['label_question'], max_new_tokens=max_new_tokens))\n",
        "    if i > n:\n",
        "      break\n",
        "  data_w_pred = data.select(range(len(pred))).add_column(\"pred\", pred)\n",
        "  accuracy, inaccurate_pred = evaluate_prediction(data_w_pred, evaluation=evaluation, pred_col='pred')\n",
        "  return accuracy, inaccurate_pred, pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "MOz7byBYAlgN"
      },
      "outputs": [],
      "source": [
        "def get_label_accuracy(pred, gt):\n",
        "  # gt stands for ground truth\n",
        "  correct_count = 0\n",
        "  wrong_idx = []\n",
        "  for k, (i,j) in enumerate(zip(pred, gt)):\n",
        "    if j.lower() in i.lower(9):\n",
        "      correct_count += 1\n",
        "    else:\n",
        "      wrong_idx.append(k)\n",
        "  return correct_count / len(pred), wrong_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "se02cFeO8AuO"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import re\n",
        "\n",
        "# To ignore specific warnings with regular expression patterns:\n",
        "warnings.filterwarnings('ignore', message=r\".*pad_token_id.*\")\n",
        "warnings.filterwarnings('ignore', message=r\".*decoder-only architecture.*\")\n",
        "\n",
        "# Your code here, e.g., using a model from the transformers library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqHYBbJ1BCuG"
      },
      "outputs": [],
      "source": [
        "# acc = get_label_accuracy(vali_label_pred, datac['label'][:len(vali_label_pred)])\n",
        "# acc_chat = get_label_accuracy(vali_label_w_chat_pred, data['valid']['label'][:len(vali_label_pred)])\n",
        "# print(acc)\n",
        "# print(acc_chat)\n",
        "\n",
        "%%capture\n",
        "cqa_model_acc = evaluate_model_accuracy(data['valid'], n=100, chat=False, max_new_tokens=30, evaluation=False)\n",
        "# cqa_model_chat_acc = evaluate_model_accuracy(data['valid'], n=50, chat=True, max_new_tokens=30, evaluation=False)\n",
        "print(cqa_model_acc[0])\n",
        "# print(cqa_model_chat_acc[0])`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjWNBedbMyKw",
        "outputId": "92dd874e-c381-48d2-df20-dd393870fd00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70.58823529411765\n"
          ]
        }
      ],
      "source": [
        "# print(cqa_model_acc[0])\n",
        "# print(cqa_model_chat_acc[0])\n",
        "print(cqa_model_acc[0])\n",
        "cqa_model_acc[1].to_csv('/content/drive/MyDrive/llm_output/cqa_wrong_llama3_.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMBHyS3_iai4"
      },
      "outputs": [],
      "source": [
        "anli1_model_acc = evaluate_model_accuracy(data_anli1['valid'], n=100, chat=False, max_new_tokens=30, evaluation=False)\n",
        "# anli1_model_chat_acc = evaluate_model_accuracy(data_anli1['valid'], n=50, chat=True, max_new_tokens=30, evaluation=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd7B6Nx4EQPf",
        "outputId": "b7b64149-5da4-4e8f-8e95-2be8e8ce6a44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "55.88235294117647"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "anli1_model_acc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRp4fx2wnR-k"
      },
      "outputs": [],
      "source": [
        "\n",
        "anli1_model_acc[1].to_csv('/content/drive/MyDrive/llm_output/anli1_wrong_llama3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsZ00bWkoIBP"
      },
      "outputs": [],
      "source": [
        "# esnli_model_acc = evaluate_model_accuracy(data_esnli['valid'], n=50, chat=False, max_new_tokens=30, evaluation=False)\n",
        "\n",
        "# print(esnli_model_acc[0])\n",
        "# esnli_model_acc[1].to_csv('/content/drive/MyDrive/esnli_wrong_mistral.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c_JsIhC963Q",
        "outputId": "8a1f6aec-f85d-4767-db9e-243e7e806168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42.30769230769231\n"
          ]
        }
      ],
      "source": [
        "# print(esnli_model_acc[0])\n",
        "# # esnli_model_acc[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyaJ-Lfu5Rfl"
      },
      "source": [
        "# FT on label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "nmKcuCwi3oyj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "# import os, torch, wandb, platform, warnings\n",
        "import os, torch, platform, warnings\n",
        "\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "JnXy8QV61opB"
      },
      "outputs": [],
      "source": [
        "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
        "model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "fRguOulwL4dk"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = get_peft_model(model, peft_config)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Load tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.add_eos_token = True\n",
        "# tokenizer.add_bos_token, tokenizer.add_eos_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "pvJQvb483pE8"
      },
      "outputs": [],
      "source": [
        "# Training Arguments\n",
        "# Hyperparameters should beadjusted based on the hardware you using\n",
        "new_model = \"llama3_ft_0420\" #set the name of the new model\n",
        "output_dir = r\"/content/drive/MyDrive/llama3_ft_0421\"\n",
        "\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir= output_dir,\n",
        "    num_train_epochs=2,\n",
        "    gradient_checkpointing=True,\n",
        "    # max_steps=200,\n",
        "    # auto_find_batch_size=True,\n",
        "    per_device_train_batch_size= 2,\n",
        "    gradient_accumulation_steps= 4,\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    save_steps= 100,\n",
        "    logging_steps= 100,\n",
        "    learning_rate= 2e-4,\n",
        "    weight_decay= 0.001,\n",
        "    fp16= False,\n",
        "    bf16= False,\n",
        "    max_grad_norm= 0.3,\n",
        "    # max_steps= -1,\n",
        "    warmup_ratio= 0.3,\n",
        "    group_by_length= True,\n",
        "    save_total_limit = 1,\n",
        "    load_best_model_at_end=True,\n",
        "    evaluation_strategy='steps',\n",
        "    save_strategy='steps',\n",
        "    resume_from_checkpoint=True,\n",
        "    lr_scheduler_type= \"cosine\",\n",
        "    per_device_eval_batch_size=2,\n",
        "    eval_accumulation_steps=2,\n",
        "    prediction_loss_only=True,\n",
        "    # report_to=\"wandb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "irZWKFQDyILN"
      },
      "outputs": [],
      "source": [
        "# model.config.use_cache = True # silence the warnings. Please re-enable for inference!\n",
        "# get_llm_answer(data['valid'][200]['rationale_question'],max_new_tokens=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "-keuO_ZsyN5v"
      },
      "outputs": [],
      "source": [
        "# model.config.use_cache = True # silence the warnings. Please re-enable for inference!\n",
        "\n",
        "# get_llm_answer(data['valid'][6]['label_question'],max_new_tokens=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCXv-9Q9dkkI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "3410f90d-21f3-473c-a1ee-ceb06404bcb1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "/content/drive/MyDrive/colab_env/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 14/220 08:05 < 2:18:51, 0.02 it/s, Epoch 0.12/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 15/220 08:45 < 2:18:10, 0.02 it/s, Epoch 0.13/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
        "\n",
        "trainer_label = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=data['train'],\n",
        "    eval_dataset=data['valid'],\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length= None,\n",
        "    dataset_text_field=\"label_pair\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing= True,\n",
        "    # formatting_func=formatting_prompts_func,\n",
        "\n",
        ")\n",
        "trainer_label.train()\n",
        "model.config.use_cache = True # silence the warnings. Please re-enable for inference!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk7xwSoe7J9S"
      },
      "outputs": [],
      "source": [
        "# clean_gpu_cache()\n",
        "%%capture\n",
        "cqa_model_acc = evaluate_model_accuracy(data['valid'], n=50, chat=False, max_new_tokens=30, evaluation=False)\n",
        "# cqa_model_chat_acc = evaluate_model_accuracy(data['valid'], n=50, chat=True, max_new_tokens=30, evaluation=False)\n",
        "print(cqa_model_acc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp6HvjFEvtM_"
      },
      "outputs": [],
      "source": [
        "print(cqa_model_acc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/llama3_label_pretrained\")"
      ],
      "metadata": {
        "id": "kuU8oiZ_bLSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below is still in development, no need to run"
      ],
      "metadata": {
        "id": "NOsBhKZN3I2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# try to save and load adapter"
      ],
      "metadata": {
        "id": "6vl3EPT6yIqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J6oA2hVtrQm",
        "outputId": "32362bf8-5865-480d-8383-4544a8c62463"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /content/drive/MyDrive/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer_label.save_model('/content/drive/MyDrive/trainer_label_llama3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1NCioaM32mw"
      },
      "outputs": [],
      "source": [
        "model.set_adapter('/content/drive/MyDrive/trainer_label_llama3')\n",
        "model.config.use_cache = True # silence the warnings. Please re-enable for inference!\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "m57UCdII35k9",
        "outputId": "8ea3130b-8090-4693-a02e-b5828ba161d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<(c) purse>\\n\\nI would choose (c) purse because it's common sense to keep your car keys in a place where you can easily access them when\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.active_adapter()\n",
        "\n",
        "get_llm_answer(data['valid'][6]['label_question'],max_new_tokens=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lG7b8a8K7jWb",
        "outputId": "22f42bc8-81de-4c0a-cb62-dffb36b981ae"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<(c) purse>\\n\\nI would choose (c) purse because it's common sense to keep your car keys in a place where you can easily access them when\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.disable_adapters()\n",
        "get_llm_answer(data['valid'][6]['label_question'],max_new_tokens=32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5er7gGfbSig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IojcrwjQhcVD"
      },
      "outputs": [],
      "source": [
        "# del model  # Delete the model to decrease reference count\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the adapter layer"
      ],
      "metadata": {
        "id": "bsqUjv29bWFC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ab84f14e4fcd4d3f80d43d68ff75a017",
            "3533d71ea33f4a9384ed9a7e3d1efea2",
            "c03e6a724ba448dc812312583ba7535a",
            "98185e99031c4ba2988e3bf1f6559bfe",
            "f3bccb820e8747cfb6e436b06f2b87e8",
            "5b6f6dbb346e41d482ac03b1048a2926",
            "a12e2be11c624cb39df0862a1bf912f1",
            "08bd017201184b388b1584d6fbd19487",
            "f6e2cdc92a8c4027b2c503bd4d3ae749",
            "c07a7b55e9ee43ec87f8aa0bfa35c700",
            "a6726a03b40a44428d6a52daac67430d"
          ]
        },
        "id": "ug6K0A9jhumu",
        "outputId": "4710cd83-96ed-407d-bf9c-fd18ff9c29ba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab84f14e4fcd4d3f80d43d68ff75a017",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "peft_model_id = \"/content/drive/MyDrive/trainer_label_llama3\"\n",
        "# peft_model_id = args.output_dir\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  quantization_config=bnb_config,\n",
        "  device_map={\"\": 0}\n",
        "  # device_map=\"auto\",\n",
        "  # torch_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHk2tC4dii59"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "75a8215d85ef4e59a12790ebe741a920",
            "07d1d9c663e44635a5c0fa889054f969",
            "bb2b970f038f4bc389ffe3a6c9fffd35",
            "e7b9dab05b4f499e89c2247461bfb82a",
            "844d72d7c9d54f13a2da5a298d8894f4",
            "7c68a028aa7742e2a8afe24ba159dab3",
            "9831206645f44f02a6977b910ac7dabe",
            "77a7c4c70a3047ec8e35cd35631c0e80",
            "9df24747822048b9a3040021623f64b5",
            "953752b0b67244928db6fe50bd444980",
            "baedf487fa904ad5bc7a61be96fa2903"
          ]
        },
        "id": "7vVVGD_32yPO",
        "outputId": "0dd9b596-f44f-4c57-98bf-0ff10ffe3e34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75a8215d85ef4e59a12790ebe741a920",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8766 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab_env/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8766\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Setting sft parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=data['train'],\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length= None,\n",
        "    dataset_text_field=\"rationale_pair\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing= False,\n",
        "    # formatting_func=formatting_prompts_func,\n",
        "\n",
        ")\n",
        "print(len(data['train']))\n",
        "clean_gpu_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "muWPS4HG3pMG",
        "outputId": "b817f7d2-6a2b-4797-b66d-618fe3614390"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-7db431a64be0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Save the fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/colab_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1625\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1961\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2909\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2911\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/colab_env/lib/python3.10/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "# Save the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr4vBG_BuIjF"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('/content/drive/MyDrive/trainer_rationale_llama3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28nv3KRcxygG"
      },
      "outputs": [],
      "source": [
        "model.add_adapter(peft_config, '/content/drive/MyDrive/trainer_rationale_llama3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCtFlka_3CiD"
      },
      "outputs": [],
      "source": [
        "model.add_adapter(peft_config, '/content/drive/MyDrive/trainer_label_llama3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_qzHDl4142g"
      },
      "outputs": [],
      "source": [
        "model.enable_adapters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEL5aOfN3RuW"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "kXSbu0csxOmU",
        "outputId": "88ebdcb3-4ee2-4b2c-f6ab-dc35856947d1"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/None/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0;31m# Repo not found => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1239\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    322\u001b[0m             )\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-66244b71-145cce514180562e44fd2c0e;a4fa84ab-0088-4604-b1de-3f6bb5daaacf)\n\nRepository Not Found for url: https://huggingface.co/None/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-018790cc4613>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load Model with PEFT adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = AutoPeftModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mpeft_model_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/colab_env/lib/python3.10/site-packages/peft/auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m             )\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mtokenizer_exists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    483\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         ) from e\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "peft_model_id = \"/content/drive/MyDrive/trainer_rationale_llama3\"\n",
        "# peft_model_id = args.output_dir\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  quantization_config=bnb_config,\n",
        "  device_map={\"\": 0}\n",
        "  # device_map=\"auto\",\n",
        "  # torch_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48VZD31k7W1s"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "cqa_model_acc = evaluate_model_accuracy(data['valid'], n=10, chat=False, max_new_tokens=30, evaluation=False)\n",
        "# cqa_model_chat_acc = evaluate_model_accuracy(data['valid'], n=50, chat=True, max_new_tokens=30, evaluation=False)\n",
        "print(cqa_model_acc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyk1JokYr3l_",
        "outputId": "033a3931-7842-47a9-af96-3bfb9e10a86e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "75.0\n"
          ]
        }
      ],
      "source": [
        "print(cqa_model_acc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVp4ChGS7lUE"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRx05Obq74NC"
      },
      "outputs": [],
      "source": [
        "# from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "# # Load PEFT model on CPU\n",
        "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "#     args.output_dir,\n",
        "#     torch_dtype=torch.float16,\n",
        "#     low_cpu_mem_usage=True,\n",
        "# )\n",
        "# # Merge LoRA and base model and save\n",
        "# merged_model = model.merge_and_unload()\n",
        "# merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGaUuSym78nJ"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(new_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96C-Q66R__Ob"
      },
      "outputs": [],
      "source": [
        "## use the below one to shut down kernal automatically to save credit\n",
        "\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 7806901,
          "sourceId": 67121,
          "sourceType": "competition"
        },
        {
          "datasetId": 4506214,
          "sourceId": 7747717,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4529653,
          "sourceId": 7750162,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4540226,
          "sourceId": 7762847,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4540586,
          "sourceId": 7763364,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4540601,
          "sourceId": 7763389,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4540659,
          "sourceId": 7763471,
          "sourceType": "datasetVersion"
        },
        {
          "sourceId": 164836055,
          "sourceType": "kernelVersion"
        },
        {
          "modelInstanceId": 3900,
          "sourceId": 5112,
          "sourceType": "modelInstanceVersion"
        },
        {
          "modelInstanceId": 8332,
          "sourceId": 11394,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30665,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07d1d9c663e44635a5c0fa889054f969": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c68a028aa7742e2a8afe24ba159dab3",
            "placeholder": "​",
            "style": "IPY_MODEL_9831206645f44f02a6977b910ac7dabe",
            "value": "Map: 100%"
          }
        },
        "08bd017201184b388b1584d6fbd19487": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3533d71ea33f4a9384ed9a7e3d1efea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b6f6dbb346e41d482ac03b1048a2926",
            "placeholder": "​",
            "style": "IPY_MODEL_a12e2be11c624cb39df0862a1bf912f1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5b6f6dbb346e41d482ac03b1048a2926": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75a8215d85ef4e59a12790ebe741a920": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07d1d9c663e44635a5c0fa889054f969",
              "IPY_MODEL_bb2b970f038f4bc389ffe3a6c9fffd35",
              "IPY_MODEL_e7b9dab05b4f499e89c2247461bfb82a"
            ],
            "layout": "IPY_MODEL_844d72d7c9d54f13a2da5a298d8894f4"
          }
        },
        "77a7c4c70a3047ec8e35cd35631c0e80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c68a028aa7742e2a8afe24ba159dab3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "844d72d7c9d54f13a2da5a298d8894f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "953752b0b67244928db6fe50bd444980": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98185e99031c4ba2988e3bf1f6559bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c07a7b55e9ee43ec87f8aa0bfa35c700",
            "placeholder": "​",
            "style": "IPY_MODEL_a6726a03b40a44428d6a52daac67430d",
            "value": " 4/4 [00:16&lt;00:00,  3.67s/it]"
          }
        },
        "9831206645f44f02a6977b910ac7dabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9df24747822048b9a3040021623f64b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a12e2be11c624cb39df0862a1bf912f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6726a03b40a44428d6a52daac67430d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab84f14e4fcd4d3f80d43d68ff75a017": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3533d71ea33f4a9384ed9a7e3d1efea2",
              "IPY_MODEL_c03e6a724ba448dc812312583ba7535a",
              "IPY_MODEL_98185e99031c4ba2988e3bf1f6559bfe"
            ],
            "layout": "IPY_MODEL_f3bccb820e8747cfb6e436b06f2b87e8"
          }
        },
        "baedf487fa904ad5bc7a61be96fa2903": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb2b970f038f4bc389ffe3a6c9fffd35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77a7c4c70a3047ec8e35cd35631c0e80",
            "max": 8766,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9df24747822048b9a3040021623f64b5",
            "value": 8766
          }
        },
        "c03e6a724ba448dc812312583ba7535a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08bd017201184b388b1584d6fbd19487",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6e2cdc92a8c4027b2c503bd4d3ae749",
            "value": 4
          }
        },
        "c07a7b55e9ee43ec87f8aa0bfa35c700": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7b9dab05b4f499e89c2247461bfb82a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_953752b0b67244928db6fe50bd444980",
            "placeholder": "​",
            "style": "IPY_MODEL_baedf487fa904ad5bc7a61be96fa2903",
            "value": " 8766/8766 [00:01&lt;00:00, 7742.06 examples/s]"
          }
        },
        "f3bccb820e8747cfb6e436b06f2b87e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6e2cdc92a8c4027b2c503bd4d3ae749": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "324be0d68baa4784a4d28ec00a3707fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dfb8ff664e724187a0d21c6c819a10bf",
              "IPY_MODEL_a3f7138cab8449cea028cbb626c9db36",
              "IPY_MODEL_44f65edeb8f34cf8bd28c3ebac674714"
            ],
            "layout": "IPY_MODEL_bec1e235c8a94fc8b6ef5884244a3172"
          }
        },
        "dfb8ff664e724187a0d21c6c819a10bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_609edd1858b84104ab8c311c2f71bc15",
            "placeholder": "​",
            "style": "IPY_MODEL_26403a33e75645c88808175dcc877d6e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a3f7138cab8449cea028cbb626c9db36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95c4f7e25ac545e585bdc252f161133a",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c44ba4ce93d4d17a912a4aaeda05b8c",
            "value": 4
          }
        },
        "44f65edeb8f34cf8bd28c3ebac674714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c4c49cf4b7549578bdb08a11538032a",
            "placeholder": "​",
            "style": "IPY_MODEL_e7e70f049fc04b94ab79b70f1e86f0fd",
            "value": " 4/4 [00:15&lt;00:00,  3.44s/it]"
          }
        },
        "bec1e235c8a94fc8b6ef5884244a3172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "609edd1858b84104ab8c311c2f71bc15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26403a33e75645c88808175dcc877d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95c4f7e25ac545e585bdc252f161133a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c44ba4ce93d4d17a912a4aaeda05b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c4c49cf4b7549578bdb08a11538032a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7e70f049fc04b94ab79b70f1e86f0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}